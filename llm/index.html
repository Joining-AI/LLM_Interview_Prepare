
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../stablediffusion/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.29">
    
    
      
        <title>LLM - AIDIY Wiki</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.76a95c52.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#21" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="AIDIY Wiki" class="md-header__button md-logo" aria-label="AIDIY Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            AIDIY Wiki
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              LLM
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  LLM

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../stablediffusion/" class="md-tabs__link">
        
  
    
  
  StableDiffusion

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../vit/" class="md-tabs__link">
        
  
    
  
  ViT

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="AIDIY Wiki" class="md-nav__button md-logo" aria-label="AIDIY Wiki" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    AIDIY Wiki
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    LLM
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    LLM
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 大模型 基础面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 大模型 进阶面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 大模型 微调面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 大模型 langchain面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26-llm" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 基于LLM+向量库的文档对话 面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#27-peft" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 大模型 参数高效微调(PEFT) 面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#28" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 大模型 推理面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#29" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 大模型 评测面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#210" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 大模型 强化学习面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#211" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 大模型 软硬件配置面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#212" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 大模型 训练集面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#213-token" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 Token及模型参数准备篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#214-alibi-attention-with-linear-biases" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 ALiBi (Attention with Linear Biases)篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#215-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 LLMs 位置编码篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#216" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 长度外推问题篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#217-llms-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 LLMs Tokenizer 篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#218-layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      2.18 Layer Normalization 篇
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../stablediffusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    StableDiffusion
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../vit/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ViT
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 大模型 基础面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 大模型 进阶面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 大模型 微调面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#25-langchain" class="md-nav__link">
    <span class="md-ellipsis">
      2.5 大模型 langchain面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#26-llm" class="md-nav__link">
    <span class="md-ellipsis">
      2.6 基于LLM+向量库的文档对话 面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#27-peft" class="md-nav__link">
    <span class="md-ellipsis">
      2.7 大模型 参数高效微调(PEFT) 面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#28" class="md-nav__link">
    <span class="md-ellipsis">
      2.8 大模型 推理面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#29" class="md-nav__link">
    <span class="md-ellipsis">
      2.9 大模型 评测面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#210" class="md-nav__link">
    <span class="md-ellipsis">
      2.10 大模型 强化学习面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#211" class="md-nav__link">
    <span class="md-ellipsis">
      2.11 大模型 软硬件配置面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#212" class="md-nav__link">
    <span class="md-ellipsis">
      2.12 大模型 训练集面
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#213-token" class="md-nav__link">
    <span class="md-ellipsis">
      2.13 Token及模型参数准备篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#214-alibi-attention-with-linear-biases" class="md-nav__link">
    <span class="md-ellipsis">
      2.14 ALiBi (Attention with Linear Biases)篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#215-llms" class="md-nav__link">
    <span class="md-ellipsis">
      2.15 LLMs 位置编码篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#216" class="md-nav__link">
    <span class="md-ellipsis">
      2.16 长度外推问题篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#217-llms-tokenizer" class="md-nav__link">
    <span class="md-ellipsis">
      2.17 LLMs Tokenizer 篇
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#218-layer-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      2.18 Layer Normalization 篇
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  <h1>LLM</h1>

<p><a href="https://github.com/Joining-AI/LLM_Interview_Prepare"><img alt="GitHub stars" src="https://img.shields.io/github/stars/Joining-AI/LLM_Interview_Prepare?style=social" /></a></p>
<h3 id="21">2.1 大模型 基础面<a class="headerlink" href="#21" title="Permanent link">&para;</a></h3>
<p>2.1.1 目前主流的开源模型体系有哪些?</p>
<p>2.1.2 prefix LM 和 causal LM 区别是什么?</p>
<p>2.1.3 涌现能力是啥原因?</p>
<p>2.1.4 大模型LLM的架构介绍?</p>
<h3 id="22">2.2 大模型 进阶面<a class="headerlink" href="#22" title="Permanent link">&para;</a></h3>
<p>2.2.1 llama 输入句子长度理论上可以无限长吗?</p>
<p>2.2.2 什么是 LLMs 复读机问题?</p>
<p>2.2.3 为什么会出现 LLMs 复读机问题?</p>
<p>2.2.4 如何缓解 LLMs 复读机问题?</p>
<p>2.2.5 什么情况用Bert模型，什么情况用LLaMA、ChatGLM类大模型，咋选?</p>
<p>2.2.6 各个专业领域是否需要各自的大模型来服务?</p>
<p>2.2.7 如何让大模型处理更长的文本?</p>
<h3 id="23">2.3 大模型 微调面<a class="headerlink" href="#23" title="Permanent link">&para;</a></h3>
<p>2.3.1 如果想要在某个模型基础上做全参数微调，究竟需要多少显存?</p>
<p>2.3.2 为什么SFT之后感觉LLM傻了?</p>
<p>2.3.3 SFT 指令微调数据如何构建?</p>
<p>2.3.4 领域模型Continue PreTrain 数据选取?</p>
<p>2.3.5 领域数据训练后，通用能力往往会有所下降，如何缓解模型遗忘通用能力?</p>
<p>2.3.6 领域模型Continue PreTrain ，如何让模型在预训练过程中就学习到更多的知识?</p>
<p>2.3.7 进行SFT操作的时候，基座模型选用Chat还是Base?</p>
<p>2.3.8 领域模型微调 指令&amp;数据输入格式 要求?</p>
<p>2.3.9 领域模型微调 领域评测集 构建?</p>
<p>2.3.10 领域模型词表扩增是不是有必要的?</p>
<p>2.3.11 如何训练自己的大模型?</p>
<p>2.3.12 训练中文大模型有啥经验?</p>
<p>2.3.13 指令微调的好处?</p>
<p>2.3.14 预训练和微调哪个阶段注入知识的?</p>
<p>2.3.15 想让模型学习某个领域或行业的知识，是应该预训练还是应该微调?</p>
<p>2.3.16 多轮对话任务如何微调模型?</p>
<p>2.3.17 微调后的模型出现能力劣化，灾难性遗忘是怎么回事?</p>
<p>2.3.18 微调模型需要多大显存?</p>
<p>2.3.19 大模型LLM进行SFT操作的时候在学习什么?</p>
<p>2.3.20 预训练和SFT操作有什么不同?</p>
<p>2.3.21 样本量规模增大，训练出现OOM错误?</p>
<p>2.3.22 大模型LLM进行SFT 如何对样本进行优化?</p>
<p>2.3.23 模型参数迭代实验?</p>
<h3 id="25-langchain">2.5 大模型 langchain面<a class="headerlink" href="#25-langchain" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>概念部分</strong></li>
</ul>
<p>2.5.1 什么是LangChain</p>
<p>2.5.2 LangChain 包含哪些核心概念?</p>
<p>2.5.3 什么是LangChain Agent?</p>
<p>2.5.4 如何使用LangChain?</p>
<p>2.5.5 LangChain 支持哪些功能?</p>
<p>2.5.6 什么是LangChain model?</p>
<p>2.5.7 LangChain 包含哪些特点?</p>
<p>2.5.8 LangChain 如何使用?</p>
<p>2.5.9 LangChain 存在哪些问题及方法方案?</p>
<p>2.5.10 LangChain 替代方案?</p>
<ul>
<li><strong>基础技术部分</strong></li>
</ul>
<p>2.5.11 LangChain 中 Components and Chains 是什么?</p>
<p>2.5.12 LangChain 中 Prompt Templates and Values 是什么?</p>
<p>2.5.13 LangChain 中 Example Selectors 是什么?</p>
<p>2.5.14 LangChain 中 Output Parsers 是什么?</p>
<p>2.5.15 LangChain 中 Indexes and Retrievers 是什么?</p>
<p>2.5.16 LangChain 中 Chat Message History 是什么?</p>
<p>2.5.17 LangChain 中 Agents and Toolkits 是什么?</p>
<p>2.5.18 LangChain 如何调用LLMs生成回复?</p>
<p>2.5.19 LangChain 如何修改提示模板?</p>
<p>2.5.20 LangChain 如何链接多个组件处理一个特定的下游任务?</p>
<p>2.5.21 LangChain 如何Embedding &amp; vector store?</p>
<p>2.5.22 LangChain 低效的令牌使用问题</p>
<p>2.5.23 LangChain 文档的问题</p>
<p>2.5.24 LangChain 太多概念容易混淆，过多的“辅助”函数问题</p>
<p>2.5.25 LangChain 行为不一致并且隐藏细节问题</p>
<p>2.5.26 LangChain 缺乏标准的可互操作数据类型问题</p>
<h3 id="26-llm">2.6 基于LLM+向量库的文档对话 面<a class="headerlink" href="#26-llm" title="Permanent link">&para;</a></h3>
<p>2.6.1 LLMs 存在模型幻觉问题，请问如何处理?</p>
<p>2.6.2 基于LLM+向量库的文档对话思路是怎么样?</p>
<p>2.6.3 基于LLM+向量库的文档对话核心技术是什么?</p>
<p>2.6.4 基于LLM+向量库的文档对话 prompt 模板如何构建?</p>
<h3 id="27-peft">2.7 大模型 参数高效微调(PEFT) 面<a class="headerlink" href="#27-peft" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>LoRA篇</strong></li>
</ul>
<p>2.7.1 什么是 LoRA?</p>
<p>2.7.2 LoRA 的思路是什么?</p>
<p>2.7.3 LoRA 的特点是什么?</p>
<ul>
<li><strong>QLoRA篇</strong></li>
</ul>
<p>2.7.4 QLoRA 的思路是怎么样的?</p>
<p>2.7.5 QLoRA 的特点是什么?</p>
<ul>
<li><strong>AdaLoRA篇</strong></li>
</ul>
<p>2.7.6 AdaLoRA 的思路是怎么样的?</p>
<p>2.7.7 LoRA权重是否可以合入原模型?</p>
<p>2.7.8 ChatGLM-6B LoRA后的权重多大?</p>
<p>2.7.9 LoRA 微调优点是什么?</p>
<p>2.7.10 LoRA微调方法为啥能加速训练?</p>
<p>2.7.11 如何在已有LoRA模型上继续训练?</p>
<ul>
<li><strong>提示学习（Prompting）</strong></li>
</ul>
<p>2.7.12 为什么需要 P-tuning?</p>
<p>2.7.13 P-tuning 思路是什么?</p>
<p>2.7.14 P-tuning 优点是什么?</p>
<p>2.7.15 P-tuning 缺点是什么?</p>
<p>2.7.16 为什么需要 指示微调（Prompt-tuning）?</p>
<p>2.7.17 指示微调（Prompt-tuning）思路是什么?</p>
<p>2.7.18 指示微调（Prompt-tuning）优点是什么?</p>
<p>2.7.19 指示微调（Prompt-tuning）缺点是什么?</p>
<p>2.7.20 指示微调（Prompt-tuning）与 Prefix-tuning 区别 是什么?</p>
<p>2.7.21 指示微调（Prompt-tuning）与 fine-tuning 区别 是什么?</p>
<p>2.7.22 提示学习（Prompting）有哪些方法，能不能稍微介绍一下它们?</p>
<ul>
<li><strong>前缀微调（Prefix-tuning）篇</strong></li>
</ul>
<p>2.7.23 为什么需要 前缀微调（Prefix-tuning）?</p>
<p>2.7.24 前缀微调（Prefix-tuning）思路是什么?</p>
<p>2.7.25 前缀微调（Prefix-tuning）的优点是什么?</p>
<p>2.7.26 前缀微调（Prefix-tuning）的缺点是什么?</p>
<ul>
<li><strong>适配器微调（Adapter-tuning）篇</strong></li>
</ul>
<p>2.7.27 为什么 需要 适配器微调（Adapter-tuning）?</p>
<p>2.7.28 适配器微调（Adapter-tuning）思路?</p>
<p>2.7.29 适配器微调（Adapter-tuning）特点是什么?</p>
<p>2.7.30 AdapterFusion 思路 是什么?</p>
<p>2.7.31 AdapterDrop 思路 是什么?</p>
<p>2.7.32 AdapterDrop 特点 是什么?</p>
<p>2.7.33 MAM Adapter 思路 是什么?</p>
<p>2.7.34 MAM Adapter 特点 是什么?</p>
<h3 id="28">2.8 大模型 推理面<a class="headerlink" href="#28" title="Permanent link">&para;</a></h3>
<p>2.8.1 为什么大模型推理时显存涨的那么多还一直占着?</p>
<p>2.8.2 大模型在gpu和cpu上推理速度如何?</p>
<p>2.8.3 推理速度上，int8和fp16比起来怎么样?</p>
<p>2.8.4 大模型有推理能力吗?</p>
<p>2.8.5 大模型生成时的参数怎么设置?</p>
<p>2.8.6 有哪些省内存的大语言模型训练/微调/推理方法?</p>
<p>2.8.7 如何让大模型输出合规化</p>
<p>2.8.8 应用模式变更</p>
<h3 id="29">2.9 大模型 评测面<a class="headerlink" href="#29" title="Permanent link">&para;</a></h3>
<p>2.9.1 大模型怎么评测?</p>
<p>2.9.2 大模型的honest原则是如何实现的?</p>
<p>2.9.3 模型如何判断回答的知识是训练过的已知的知识，怎么训练这种能力?</p>
<h3 id="210">2.10 大模型 强化学习面<a class="headerlink" href="#210" title="Permanent link">&para;</a></h3>
<p>2.10.1 奖励模型需要和基础模型一致吗?</p>
<p>2.10.2 RLHF 在实践过程中存在哪些不足?</p>
<p>2.10.3 如何解决 人工产生的偏好数据集成本较高，很难量产问题?</p>
<p>2.10.4 如何解决三个阶段的训练（SFT-&gt;RM-&gt;PPO）过程较长，更新迭代较慢问题?</p>
<p>2.10.5 如何解决 PPO 的训练过程同时存在4个模型（2训练，2推理），对计算资源的要求较高问题?</p>
<h3 id="211">2.11 大模型 软硬件配置面<a class="headerlink" href="#211" title="Permanent link">&para;</a></h3>
<p>2.11.1 介绍一下 FFN 块 计算公式?</p>
<p>2.11.2 介绍一下 GeLU 计算公式?</p>
<p>2.11.3 介绍一下 Swish 计算公式?</p>
<p>2.11.4 介绍一下 使用 GLU 线性门控单元的 FFN 块 计算公式?</p>
<p>2.11.5 介绍一下 使用 GeLU 的 GLU 块 计算公式?</p>
<p>2.11.6 介绍一下 使用 Swish 的 GLU 块 计算公式?</p>
<p>2.11.7 各LLMs 都使用哪种激活函数?</p>
<h3 id="212">2.12 大模型 训练集面<a class="headerlink" href="#212" title="Permanent link">&para;</a></h3>
<p>2.12.1 SFT（有监督微调）的数据集格式?</p>
<p>2.12.2 RM（奖励模型）的数据格式?</p>
<p>2.12.3 PPO（强化学习）的数据格式?</p>
<p>2.12.4 找数据集哪里找?</p>
<p>2.12.5 微调需要多少条数据?</p>
<p>2.12.6 有哪些大模型的训练集?</p>
<p>2.12.7 进行领域大模型预训练应用哪些数据集比较好?</p>
<p>2.12.8 如何给LLM注入领域知识?</p>
<p>2.12.9 如果想要快速体验各种模型，该怎么办?</p>
<h3 id="213-token">2.13 Token及模型参数准备篇<a class="headerlink" href="#213-token" title="Permanent link">&para;</a></h3>
<p>2.13.1 预训练数据 Token 重复 是否影响 模型性能?</p>
<p>2.13.2 SFT需要训练Token数?</p>
<h3 id="214-alibi-attention-with-linear-biases">2.14 ALiBi (Attention with Linear Biases)篇<a class="headerlink" href="#214-alibi-attention-with-linear-biases" title="Permanent link">&para;</a></h3>
<p>2.14.1 ALiBi (Attention with Linear Biases) 思路是什么?</p>
<p>2.14.2 ALiBi (Attention with Linear Biases) 的偏置矩阵是什么?有什么作用?</p>
<p>2.14.3 ALiBi (Attention with Linear Biases) 有什么优点?</p>
<p>2.14.4 ALiBi (Attention with Linear Biases) 被哪些 LLMs 应用?</p>
<h3 id="215-llms">2.15 LLMs 位置编码篇<a class="headerlink" href="#215-llms" title="Permanent link">&para;</a></h3>
<p>2.15.1 什么是位置编码?</p>
<p>2.15.1.1 什么是绝对位置编码?</p>
<p>2.15.1.2 什么是相对位置编码?</p>
<ul>
<li><strong>旋转位置编码 RoPE篇</strong></li>
</ul>
<p>2.15.2 旋转位置编码 RoPE 思路是什么?</p>
<p>2.15.3 推导一下 旋转位置编码 RoPE ?</p>
<p>2.15.4 旋转位置编码 RoPE 有什么优点?</p>
<p>2.15.5 旋转位置编码 RoPE 被哪些 LLMs 应用?</p>
<h3 id="216">2.16 长度外推问题篇<a class="headerlink" href="#216" title="Permanent link">&para;</a></h3>
<p>2.16.1 什么是 长度外推问题?</p>
<p>2.16.2 长度外推问题 的 解决方法 有哪些?</p>
<h3 id="217-llms-tokenizer">2.17 LLMs Tokenizer 篇<a class="headerlink" href="#217-llms-tokenizer" title="Permanent link">&para;</a></h3>
<p>2.17.1 Byte-Pair Encoding(BPE)篇</p>
<p>2.17.1.1 Byte-Pair Encoding(BPE) 如何构建词典?</p>
<p>2.17.2 WordPiece 篇</p>
<p>2.17.2.1 WordPiece 与 BPE 异同点是什么?</p>
<p>2.17.3 SentencePiece 篇</p>
<p>2.17.3.1 简单介绍一下 SentencePiece 思路?</p>
<ul>
<li><strong>对比篇</strong></li>
</ul>
<p>2.17.4 举例介绍一下 不同 大模型LLMs 的分词方式?</p>
<p>2.17.5 介绍一下 不同 大模型LLMs 的分词方式 的区别?</p>
<h3 id="218-layer-normalization">2.18 Layer Normalization 篇<a class="headerlink" href="#218-layer-normalization" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Layer Norm 篇</strong></li>
</ul>
<p>2.18.1 Layer Norm 的计算公式写一下?</p>
<ul>
<li><strong>RMS Norm 篇 （均方根 Norm）</strong></li>
</ul>
<p>2.18.2 RMS Norm 的计算公式写一下?</p>
<p>2.18.3 RMS Norm 相比于 Layer Norm 有什么特点?</p>
<ul>
<li><strong>Deep Norm 篇</strong></li>
</ul>
<p>2.18.4 Deep Norm 有什么优点?</p>
<p>2.18.5 Deep Norm 思路?</p>
<p>2.18.6 写一下 Deep Norm 代码实现?</p>
<ul>
<li>
<p><strong>Layer normalization-方法篇</strong></p>
</li>
<li>
<p><strong>Layer normalization-位置篇</strong></p>
</li>
</ul>
<p>2.18.7 LN 在 LLMs 中的不同位置 有什么区别么?如果有，能介绍一下区别么?</p>
<ul>
<li><strong>Layer normalization 对比篇</strong></li>
</ul>
<p>2.18.8 LLMs 各模型分别用了 哪种 Layer normalization?</p>
<p><a href="https://github.com/Joining-AI/LLM_Interview_Prepare"><img alt="GitHub stars" src="https://img.shields.io/github/stars/Joining-AI/LLM_Interview_Prepare?style=social" /></a></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/Joining-AI/LLM_Interview_Prepare" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.sections", "navigation.tabs"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
        <script src="../javascripts/mathjax_config.js"></script>
      
    
  </body>
</html>